"""
FIXED integrated_sig_verifier.py - Complete replacement to fix broken similarity calculations
This version properly discriminates between different signatures instead of always returning 97-100%
UPDATED: Completely rewritten geometric feature extraction for proper discrimination
"""

import os
import cv2
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Model, Sequential, load_model
from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Dense, Flatten, 
                                   Lambda, BatchNormalization, Dropout, LSTM, 
                                   Bidirectional)
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications.imagenet_utils import preprocess_input
from tensorflow.keras.optimizers import Adam
from skimage.feature import graycomatrix, graycoprops, local_binary_pattern
from skimage.measure import moments_hu
from skimage import morphology, measure
from scipy.spatial.distance import cosine, euclidean
from scipy import ndimage
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
import mahotas as mt
import joblib
import time

# Define the distance function exactly as in the original system
@tf.keras.utils.register_keras_serializable(package='Custom')
def euclidean_distance(vectors):
    """
    Calculate Euclidean distance between vectors
    Used as custom layer in Siamese network
    """
    x, y = vectors
    sum_square = tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True)
    return tf.sqrt(tf.maximum(sum_square, tf.keras.backend.epsilon()))

class ImprovedSignaturePreprocessor:
    """
    Advanced preprocessing for extracting meaningful sequential features from signature images
    (Same as used in the improved LSTM training)
    """
    
    def __init__(self, target_size=(256, 128), sequence_length=128):
        self.target_size = target_size
        self.sequence_length = sequence_length
        
    def preprocess_signature(self, img_path, return_original=False):
        """Enhanced preprocessing with noise reduction and normalization"""
        try:
            # Load image
            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
            if img is None:
                raise ValueError(f"Could not load image: {img_path}")
            
            original = img.copy()
            
            # Advanced preprocessing pipeline
            # 1. Resize maintaining aspect ratio
            img = self._resize_with_padding(img, self.target_size)
            
            # 2. Noise reduction
            img = cv2.bilateralFilter(img, 9, 75, 75)
            
            # 3. Adaptive thresholding for better binarization
            img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                       cv2.THRESH_BINARY_INV, 11, 2)
            
            # 4. Morphological operations to clean up
            kernel = np.ones((2,2), np.uint8)
            img = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)
            img = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)
            
            # 5. Skeletonization for better feature extraction
            img = morphology.skeletonize(img > 0).astype(np.uint8) * 255
            
            # 6. Normalize
            img = img.astype(np.float32) / 255.0
            
            if return_original:
                return img, original
            return img
            
        except Exception as e:
            print(f"Error preprocessing {img_path}: {e}")
            # Return empty image of correct size
            empty_img = np.zeros(self.target_size, dtype=np.float32)
            if return_original:
                return empty_img, empty_img
            return empty_img
    
    def _resize_with_padding(self, img, target_size):
        """Resize image while maintaining aspect ratio with padding"""
        h, w = img.shape
        target_h, target_w = target_size
        
        # Calculate scaling factor
        scale = min(target_w/w, target_h/h)
        
        # New dimensions
        new_w = int(w * scale)
        new_h = int(h * scale)
        
        # Resize
        resized = cv2.resize(img, (new_w, new_h))
        
        # Create padded image
        padded = np.ones(target_size, dtype=img.dtype) * 255  # White background
        
        # Calculate padding
        pad_x = (target_w - new_w) // 2
        pad_y = (target_h - new_h) // 2
        
        # Place resized image in center
        padded[pad_y:pad_y+new_h, pad_x:pad_x+new_w] = resized
        
        return padded
    
    def extract_enhanced_features(self, img_path, feature_dim=16):
        """
        Extract enhanced sequential features using multiple methods
        """
        # Preprocess image
        img = self.preprocess_signature(img_path)
        
        # Combine multiple feature extraction methods
        features = []
        
        # Method 1: Skeleton traversal features
        skeleton_features = self._extract_skeleton_features(img, feature_dim//4)
        features.append(skeleton_features)
        
        # Method 2: Grid-based scanning features  
        grid_features = self._extract_grid_features(img, feature_dim//4)
        features.append(grid_features)
        
        # Method 3: Stroke-based features
        stroke_features = self._extract_stroke_features(img, feature_dim//4)
        features.append(stroke_features)
        
        # Method 4: Pressure and thickness features
        pressure_features = self._extract_pressure_features(img, feature_dim//4)
        features.append(pressure_features)
        
        # Concatenate all features
        combined_features = np.concatenate(features, axis=1)
        
        return combined_features
    
    def _extract_skeleton_features(self, img, feature_dim):
        """Extract features by traversing the skeleton of the signature"""
        # Find skeleton points
        skeleton_points = np.where(img > 0.5)
        
        if len(skeleton_points[0]) == 0:
            return np.zeros((self.sequence_length, feature_dim))
        
        # Create coordinate pairs
        points = list(zip(skeleton_points[1], skeleton_points[0]))  # (x, y)
        
        # Sort points to create a traversal path (simple left-to-right, top-to-bottom)
        points.sort(key=lambda p: (p[1], p[0]))
        
        # Extract features along the path
        features = np.zeros((self.sequence_length, feature_dim))
        
        if len(points) == 0:
            return features
        
        # Sample points uniformly
        if len(points) >= self.sequence_length:
            indices = np.linspace(0, len(points)-1, self.sequence_length).astype(int)
            sampled_points = [points[i] for i in indices]
        else:
            # Repeat last point if not enough points
            sampled_points = points + [points[-1]] * (self.sequence_length - len(points))
        
        # Extract features for each point
        for i, (x, y) in enumerate(sampled_points):
            if i >= self.sequence_length:
                break
                
            # Normalized position
            features[i, 0] = x / img.shape[1]
            features[i, 1] = y / img.shape[0]
            
            # Local direction (if not first point)
            if i > 0:
                prev_x, prev_y = sampled_points[i-1]
                dx = x - prev_x
                dy = y - prev_y
                
                # Direction angle
                angle = np.arctan2(dy, dx)
                features[i, 2] = (angle + np.pi) / (2 * np.pi)
                
                # Distance from previous point
                dist = np.sqrt(dx**2 + dy**2)
                features[i, 3] = min(dist / 10.0, 1.0)  # Normalize and cap
        
        return features
    
    def _extract_grid_features(self, img, feature_dim):
        """Extract features by scanning image in a grid pattern"""
        features = np.zeros((self.sequence_length, feature_dim))
        
        h, w = img.shape
        grid_h = int(np.sqrt(self.sequence_length))
        grid_w = self.sequence_length // grid_h
        
        # Ensure we don't exceed sequence length
        grid_size = min(grid_h * grid_w, self.sequence_length)
        
        cell_h = h // grid_h
        cell_w = w // grid_w
        
        for i in range(grid_size):
            row = i // grid_w
            col = i % grid_w
            
            # Extract region
            start_y = row * cell_h
            end_y = min((row + 1) * cell_h, h)
            start_x = col * cell_w
            end_x = min((col + 1) * cell_w, w)
            
            region = img[start_y:end_y, start_x:end_x]
            
            # Extract features from region
            features[i, 0] = np.mean(region)  # Average intensity
            features[i, 1] = np.std(region)   # Intensity variation
            features[i, 2] = np.sum(region > 0.5) / region.size  # Ink density
            features[i, 3] = col / grid_w  # Relative x position
            
        return features
    
    def _extract_stroke_features(self, img, feature_dim):
        """Extract stroke-based features"""
        features = np.zeros((self.sequence_length, feature_dim))
        
        # Find connected components (strokes)
        labeled_img = measure.label(img > 0.5)
        regions = measure.regionprops(labeled_img)
        
        if not regions:
            return features
        
        # Sort regions by area (largest first)
        regions.sort(key=lambda r: r.area, reverse=True)
        
        # Extract features from each region
        for i, region in enumerate(regions[:self.sequence_length]):
            # Stroke properties
            features[i, 0] = region.area / (img.shape[0] * img.shape[1])  # Relative area
            features[i, 1] = region.eccentricity  # Shape eccentricity
            features[i, 2] = region.centroid[1] / img.shape[1]  # Normalized centroid x
            features[i, 3] = region.centroid[0] / img.shape[0]  # Normalized centroid y
            
        return features
    
    def _extract_pressure_features(self, img, feature_dim):
        """Extract simulated pressure and thickness features"""
        features = np.zeros((self.sequence_length, feature_dim))
        
        # Create distance transform to simulate pressure
        binary = (img > 0.5).astype(np.uint8)
        if np.sum(binary) == 0:
            return features
        
        # Distance transform gives thickness information
        dist_transform = cv2.distanceTransform(binary, cv2.DIST_L2, 5)
        
        # Scan along signature path
        y_coords, x_coords = np.where(binary > 0)
        
        if len(x_coords) == 0:
            return features
        
        # Sample points
        if len(x_coords) >= self.sequence_length:
            indices = np.linspace(0, len(x_coords)-1, self.sequence_length).astype(int)
        else:
            indices = list(range(len(x_coords))) + [len(x_coords)-1] * (self.sequence_length - len(x_coords))
        
        for i, idx in enumerate(indices):
            if i >= self.sequence_length:
                break
                
            x, y = x_coords[idx], y_coords[idx]
            
            # Simulated pressure (based on distance transform)
            features[i, 0] = dist_transform[y, x] / np.max(dist_transform) if np.max(dist_transform) > 0 else 0
            
            # Local thickness variation
            local_region = dist_transform[max(0, y-2):y+3, max(0, x-2):x+3]
            features[i, 1] = np.std(local_region) if local_region.size > 0 else 0
            
            # Gradient information
            if i > 0:
                prev_x, prev_y = x_coords[indices[i-1]], y_coords[indices[i-1]]
                features[i, 2] = abs(dist_transform[y, x] - dist_transform[prev_y, prev_x])
            
            # Position
            features[i, 3] = i / self.sequence_length
        
        return features

class SignatureFeatureExtractor:
    def __init__(self, img_size=(224, 224)):
        self.img_size = img_size
        self.cnn_model = self._load_cnn_model()

    def _load_cnn_model(self):
        """Initialize VGG16 model for feature extraction"""
        base_model = VGG16(weights='imagenet', include_top=False)
        return Model(inputs=base_model.input, 
                    outputs=base_model.get_layer('block5_pool').output)

    def extract_cnn_features(self, img_path):
        """Extract features using VGG16 CNN"""
        try:
            img = image.load_img(img_path, target_size=self.img_size, color_mode='rgb')
            img_array = image.img_to_array(img)
            img_array = np.expand_dims(img_array, axis=0)
            img_array = preprocess_input(img_array)
            features = self.cnn_model.predict(img_array, verbose=0)
            return features.flatten()
        except Exception as e:
            print(f"Error extracting CNN features: {e}")
            return None

    def extract_texture_features(self, img_path):
        """FIXED: Extract texture features with proper discrimination"""
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            return np.zeros(30)

        # Create mask to focus only on signature area (KEY FIX)
        _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        
        # Dilate to get signature neighborhood
        kernel = np.ones((5,5), np.uint8)
        signature_mask = cv2.dilate(binary, kernel, iterations=2)
        
        # Extract texture only from signature area
        masked_img = cv2.bitwise_and(img, img, mask=signature_mask)
        
        # Get only signature pixels
        signature_pixels = masked_img[signature_mask > 0]
        if len(signature_pixels) < 10:
            return np.zeros(30)
        
        # GLCM on signature area only - CRITICAL FIX
        # Create small texture patch from signature pixels
        patch_size = min(32, int(np.sqrt(len(signature_pixels))))
        if patch_size < 8:
            return np.zeros(30)
        
        # Reshape signature pixels into small image
        texture_patch = signature_pixels[:patch_size*patch_size].reshape(patch_size, patch_size)
        
        # GLCM with multiple parameters for better discrimination
        distances = [1, 2]
        angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]
        
        glcm_features = []
        try:
            for dist in distances:
                glcm = graycomatrix(texture_patch, [dist], angles, 
                                  levels=32, symmetric=True, normed=True)
                
                # Extract discriminative properties
                for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy']:
                    values = graycoprops(glcm, prop)[0]
                    glcm_features.extend(values)
        except:
            glcm_features = [0] * 16
        
        # LBP on signature area - more discriminative
        lbp = local_binary_pattern(masked_img, 16, 2, method='uniform')
        lbp_signature = lbp[signature_mask > 0]
        
        if len(lbp_signature) > 0:
            # Create histogram with more bins for better discrimination
            hist, _ = np.histogram(lbp_signature, bins=14, range=(0, 14))
            lbp_features = hist.astype(float) / (hist.sum() + 1e-7)
        else:
            lbp_features = np.zeros(14)
        
        # Combine features
        all_features = np.concatenate([glcm_features[:16], lbp_features])
        return all_features[:30]

    def extract_geometric_features(self, img_path):
        """
        COMPLETELY REWRITTEN: Extract highly discriminative signature-specific geometric features
        This new implementation focuses on signature characteristics that vary between individuals
        """
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            return np.zeros(20)

        # Preprocess image
        _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        
        # Remove noise
        kernel = np.ones((2,2), np.uint8)
        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
        
        features = []
        
        # === SIGNATURE ENVELOPE ANALYSIS ===
        # Find signature bounding box
        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if not contours:
            return np.zeros(20)
        
        # Get overall signature bounding box
        all_points = np.vstack([contour.squeeze() for contour in contours if len(contour) > 0])
        if len(all_points) == 0:
            return np.zeros(20)
            
        x_min, y_min = np.min(all_points, axis=0)
        x_max, y_max = np.max(all_points, axis=0)
        
        sig_width = x_max - x_min
        sig_height = y_max - y_min
        
        # Feature 1-3: Signature envelope ratios (highly discriminative)
        img_height, img_width = img.shape
        width_ratio = sig_width / img_width if img_width > 0 else 0
        height_ratio = sig_height / img_height if img_height > 0 else 0
        aspect_ratio = sig_width / sig_height if sig_height > 0 else 0
        
        features.extend([width_ratio, height_ratio, aspect_ratio])
        
        # === SIGNATURE MASS DISTRIBUTION ===
        # Divide signature into zones and analyze mass distribution
        sig_region = binary[y_min:y_max+1, x_min:x_max+1] if sig_height > 0 and sig_width > 0 else binary
        
        if sig_region.size == 0:
            features.extend([0] * 6)
        else:
            h, w = sig_region.shape
            
            # Feature 4-6: Horizontal mass distribution (left, center, right thirds)
            left_third = np.sum(sig_region[:, :w//3]) if w > 2 else 0
            middle_third = np.sum(sig_region[:, w//3:2*w//3]) if w > 2 else 0
            right_third = np.sum(sig_region[:, 2*w//3:]) if w > 2 else 0
            total_mass = left_third + middle_third + right_third
            
            if total_mass > 0:
                left_ratio = left_third / total_mass
                middle_ratio = middle_third / total_mass  
                right_ratio = right_third / total_mass
            else:
                left_ratio = middle_ratio = right_ratio = 0
                
            features.extend([left_ratio, middle_ratio, right_ratio])
        
        # === STROKE DIRECTION ANALYSIS ===
        # Analyze dominant stroke directions using gradient analysis
        if sig_region.size > 0:
            # Calculate gradients
            grad_x = cv2.Sobel(sig_region.astype(np.float32), cv2.CV_32F, 1, 0, ksize=3)
            grad_y = cv2.Sobel(sig_region.astype(np.float32), cv2.CV_32F, 0, 1, ksize=3)
            
            # Calculate angles where there's significant gradient
            magnitude = np.sqrt(grad_x**2 + grad_y**2)
            mask = magnitude > np.percentile(magnitude, 75)  # Only strong gradients
            
            if np.any(mask):
                angles = np.arctan2(grad_y[mask], grad_x[mask])
                
                # Feature 7-9: Dominant stroke directions
                # Convert to 0-180 degree range and create histogram
                angles_deg = (angles * 180 / np.pi) % 180
                hist, _ = np.histogram(angles_deg, bins=[0, 45, 90, 135, 180])
                hist = hist.astype(float) / (np.sum(hist) + 1e-8)
                
                # Dominant directions: horizontal-like, diagonal, vertical-like
                horizontal_tendency = hist[0] + hist[3]  # 0-45 and 135-180
                diagonal_tendency = hist[1] + hist[2]    # 45-90 and 90-135  
                direction_entropy = -np.sum(hist * np.log(hist + 1e-8))
                
                features.extend([horizontal_tendency, diagonal_tendency, direction_entropy])
            else:
                features.extend([0, 0, 0])
        else:
            features.extend([0, 0, 0])
        
        # === SIGNATURE COMPLEXITY MEASURES ===
        # Feature 10-12: Signature complexity and density variations
        if len(contours) > 0:
            # Total perimeter vs area ratio (complexity measure)
            total_area = np.sum(binary) / 255
            total_perimeter = sum(cv2.arcLength(contour, True) for contour in contours)
            
            complexity_ratio = total_perimeter / (total_area + 1e-8)
            
            # Number of significant components (strokes/characters)
            significant_contours = [c for c in contours if cv2.contourArea(c) > 20]
            num_components = len(significant_contours)
            
            # Average component size variation
            if len(significant_contours) > 1:
                areas = [cv2.contourArea(c) for c in significant_contours]
                area_std = np.std(areas) / (np.mean(areas) + 1e-8)
            else:
                area_std = 0
            
            features.extend([
                min(complexity_ratio / 100, 1.0),  # Normalize complexity
                min(num_components / 20, 1.0),     # Normalize component count
                min(area_std, 1.0)                 # Component size variation
            ])
        else:
            features.extend([0, 0, 0])
        
        # === SPATIAL FREQUENCY ANALYSIS ===
        # Feature 13-15: How signature varies across space
        if sig_region.size > 0 and sig_height > 10 and sig_width > 10:
            # Horizontal projection (sum along vertical axis)
            h_projection = np.sum(sig_region, axis=0)
            # Vertical projection (sum along horizontal axis)  
            v_projection = np.sum(sig_region, axis=1)
            
            # Calculate variation in projections
            h_variation = np.std(h_projection) / (np.mean(h_projection) + 1e-8)
            v_variation = np.std(v_projection) / (np.mean(v_projection) + 1e-8)
            
            # Find peaks in projections (character boundaries)
            h_peaks = len([i for i in range(1, len(h_projection)-1) 
                          if h_projection[i] > h_projection[i-1] and h_projection[i] > h_projection[i+1]
                          and h_projection[i] > np.mean(h_projection)])
            
            features.extend([
                min(h_variation, 2.0),              # Horizontal density variation
                min(v_variation, 2.0),              # Vertical density variation  
                min(h_peaks / 10, 1.0)              # Number of character-like peaks
            ])
        else:
            features.extend([0, 0, 0])
        
        # === SIGNATURE MOMENT ANALYSIS ===
        # Feature 16-18: Geometric moments for shape analysis
        if np.sum(binary) > 0:
            # Calculate central moments
            M = cv2.moments(binary)
            
            if M['m00'] > 0:
                # Centralized coordinates
                cx = M['m10'] / M['m00']
                cy = M['m01'] / M['m00']
                
                # Calculate normalized central moments
                mu20 = M['mu20'] / M['m00']
                mu02 = M['mu02'] / M['m00'] 
                mu11 = M['mu11'] / M['m00']
                
                # Shape characteristics from moments
                eccentricity = ((mu20 - mu02)**2 + 4*mu11**2) / (mu20 + mu02 + 1e-8)
                skewness_x = abs(M['mu30']) / (M['m00'] * (mu20)**(3/2) + 1e-8)
                skewness_y = abs(M['mu03']) / (M['m00'] * (mu02)**(3/2) + 1e-8)
                
                features.extend([
                    min(eccentricity, 1.0),
                    min(skewness_x, 1.0), 
                    min(skewness_y, 1.0)
                ])
            else:
                features.extend([0, 0, 0])
        else:
            features.extend([0, 0, 0])
        
        # === FINAL SIGNATURE DENSITY MEASURE ===
        # Feature 19-20: Overall signature density characteristics
        total_pixels = img.shape[0] * img.shape[1]
        signature_pixels = np.sum(binary) / 255
        
        overall_density = signature_pixels / total_pixels
        
        # Local density variation within signature area
        if sig_region.size > 0:
            # Divide signature into 4 quadrants and measure density variation
            h, w = sig_region.shape
            if h > 1 and w > 1:
                quad1 = np.sum(sig_region[:h//2, :w//2]) / (h//2 * w//2 * 255)
                quad2 = np.sum(sig_region[:h//2, w//2:]) / ((h//2) * (w - w//2) * 255)
                quad3 = np.sum(sig_region[h//2:, :w//2]) / ((h - h//2) * (w//2) * 255)
                quad4 = np.sum(sig_region[h//2:, w//2:]) / ((h - h//2) * (w - w//2) * 255)
                
                quad_densities = [quad1, quad2, quad3, quad4]
                density_variation = np.std(quad_densities)
            else:
                density_variation = 0
        else:
            density_variation = 0
        
        features.extend([overall_density, density_variation])
        
        # Ensure exactly 20 features
        features = features[:20]
        if len(features) < 20:
            features.extend([0] * (20 - len(features)))
        
        return np.array(features)

    def extract_structural_features(self, img_path):
        """COMPLETELY FIXED: Extract highly discriminative structural features"""
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            return np.zeros(15)

        _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        
        # Get skeleton
        try:
            skeleton = mt.thin(binary)
        except:
            # Fallback if mahotas fails
            kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (3,3))
            skeleton = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
        
        features = []
        
        # 1. Stroke width analysis - HIGHLY DISCRIMINATIVE
        dist_transform = cv2.distanceTransform(binary, cv2.DIST_L2, 5)
        skeleton_mask = skeleton > 0
        
        if np.any(skeleton_mask):
            stroke_widths = dist_transform[skeleton_mask] * 2
            features.extend([
                np.mean(stroke_widths),      # Average stroke width
                np.std(stroke_widths),       # Stroke width variation  
                np.max(stroke_widths),       # Maximum stroke width
                np.percentile(stroke_widths, 25),  # 25th percentile
                np.percentile(stroke_widths, 75),  # 75th percentile
            ])
        else:
            features.extend([0, 0, 0, 0, 0])
        
        # 2. Signature density distribution - VERY DISCRIMINATIVE  
        h, w = skeleton.shape
        
        # Divide into 6 regions instead of 4 for better discrimination
        regions = [
            skeleton[:h//3, :w//2],          # Top-left
            skeleton[:h//3, w//2:],          # Top-right
            skeleton[h//3:2*h//3, :w//2],    # Middle-left  
            skeleton[h//3:2*h//3, w//2:],    # Middle-right
            skeleton[2*h//3:, :w//2],        # Bottom-left
            skeleton[2*h//3:, w//2:]         # Bottom-right
        ]
        
        densities = [np.sum(region) / region.size for region in regions]
        features.extend(densities)
        
        # 3. Endpoint and junction analysis with spatial info
        kernel = np.ones((3,3), np.uint8)
        kernel[1,1] = 10
        filtered = cv2.filter2D(skeleton.astype(float), -1, kernel)
        
        endpoints = np.where(filtered == 11)
        junctions = np.where(filtered >= 13)
        
        # Count and spatial distribution
        endpoint_count = len(endpoints[0])
        junction_count = len(junctions[0])
        
        # Spatial spread of endpoints (more discriminative)
        if endpoint_count > 1:
            endpoint_spread_x = np.std(endpoints[1]) / w
            endpoint_spread_y = np.std(endpoints[0]) / h
        else:
            endpoint_spread_x = endpoint_spread_y = 0
        
        features.extend([endpoint_count, junction_count, endpoint_spread_x, endpoint_spread_y])
        
        # Ensure exactly 15 features
        features = features[:15]
        features.extend([0] * (15 - len(features)))
        
        return np.array(features)

class DynamicSignatureVerifier:
    def __init__(self, siamese_model_path=None):
        self.feature_extractor = SignatureFeatureExtractor()
        self.improved_lstm_model = None  # Will hold the trained improved LSTM
        self.improved_preprocessor = ImprovedSignaturePreprocessor(sequence_length=128)
        self.lstm_scaler = None  # Will hold the scaler for LSTM features
        self.siamese_network = None  # Will be loaded later
        
        # OPTIMIZED MODEL WEIGHTS - Based on performance analysis
        self.weights = {
            'cnn': 0.65,        # 65% - Your best performing feature
            'siamese': 0.25,    # 25% - Second most reliable  
            'lstm': 0.06,       # 6% - Good but inconsistent
            'structural': 0.01, # 1% - Decent but not critical
            'texture': 0.02,    # 2% - Improved but still problematic
            'geometric': 0.01   # 1% - Now properly discriminative but minimal weight
        }
        
        # OPTIMIZED THRESHOLD - Provides perfect separation on test data
        self.threshold = 0.587  # 58.7% threshold for optimal discrimination

        # Try to load the siamese model if path provided
        if siamese_model_path:
            self.load_models(None, siamese_model_path)

    def extract_dynamic_features(self, img_path):
        """Extract dynamic features for original LSTM (keeping for compatibility)"""
        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        if img is None:
            return np.zeros((64, 32))

        # Preprocessing
        img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)
        _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
        
        # Try to use OpenCV ximgproc for thinning, fall back to mahotas if not available
        try:
            skeleton = cv2.ximgproc.thinning(binary)
        except:
            skeleton = mt.thin(binary)
        
        # Extract contour points
        contours, _ = cv2.findContours(skeleton, cv2.RETR_LIST, 
                                     cv2.CHAIN_APPROX_NONE)
        
        if not contours:
            return np.zeros((64, 32))

        # Combine and sort points - handle potential empty contours
        valid_contours = [contour.squeeze() for contour in contours 
                          if len(contour) > 0 and contour.ndim > 1]
        
        if not valid_contours:
            return np.zeros((64, 32))
            
        all_points = np.vstack(valid_contours)
        all_points = all_points[all_points[:, 0].argsort()]

        # Extract dynamic features
        features = []
        for i in range(len(all_points) - 1):
            point = all_points[i]
            next_point = all_points[i + 1]

            velocity = np.linalg.norm(next_point - point)
            angle = np.arctan2(next_point[1] - point[1], 
                             next_point[0] - point[0])
            
            # Get a window around the point for local features - handle edge cases
            y_min = max(point[1]-5, 0)
            y_max = min(point[1]+6, img.shape[0])
            x_min = max(point[0]-5, 0)
            x_max = min(point[0]+6, img.shape[1])
            
            window = img[y_min:y_max, x_min:x_max]
            
            # Compute local features with checks for empty windows
            if window.size > 0:
                local_mean = np.mean(window)
                local_std = np.std(window)
            else:
                local_mean = 0
                local_std = 0
            
            local_features = [
                local_mean,
                local_std,
                velocity,
                angle,
                point[0]/img.shape[1],
                point[1]/img.shape[0],
            ]
            
            features.append(local_features)

        # Process features
        features = np.array(features)
        if len(features) > 64:
            indices = np.linspace(0, len(features)-1, 64, dtype=int)
            features = features[indices]
        else:
            # Handle case where features might be empty
            feature_dim = 6 if len(features) > 0 and features.shape[1] > 0 else 32
            padding = np.zeros((64 - len(features), feature_dim))
            features = np.vstack([features, padding]) if len(features) > 0 else padding

        # Expand features to full dimension
        expanded_features = np.zeros((64, 32))
        expanded_features[:, :features.shape[1]] = features
        
        return expanded_features

    def calculate_similarity(self, vector1, vector2, feature_type='default'):
        """FIXED similarity calculation - keeps CNN working, fixes others"""
        if vector1 is None or vector2 is None:
            return 0.0
            
        # Convert to numpy arrays
        v1 = np.array(vector1).flatten()
        v2 = np.array(vector2).flatten()
        
        # Ensure same length
        min_len = min(len(v1), len(v2))
        v1 = v1[:min_len]
        v2 = v2[:min_len]
        
        if min_len == 0 or np.all(v1 == 0) and np.all(v2 == 0):
            return 0.0
        
        # FEATURE-SPECIFIC SIMILARITY CALCULATIONS        
        if feature_type == 'geometric':
            # FIXED: Highly discriminative geometric similarity calculation
            # Removed problematic per-vector normalization that was destroying discrimination
            
            # Feature importance weights (no normalization needed)
            weights = np.array([3.0, 3.0, 2.5,  # envelope features (most discriminative)
                               2.0, 2.0, 2.0,   # mass distribution (very important)
                               1.5, 1.5, 1.0,   # stroke direction (important)
                               1.2, 1.2, 1.2,   # complexity measures
                               1.0, 1.0, 1.0,   # spatial frequency
                               0.8, 0.8, 0.8,   # geometric moments
                               0.5, 0.5])       # density measures
            weights = weights[:len(v1)]
            
            # 1. Weighted absolute differences (most reliable for geometric features)
            abs_diffs = np.abs(v1 - v2)
            weighted_abs_diffs = weights * abs_diffs
            
            # Calculate similarity based on weighted differences
            # Use MUCH more realistic expected ranges for aggressive discrimination
            expected_ranges = np.array([
                0.3, 0.3, 1.0,     # envelope: realistic variation in ratios
                0.4, 0.4, 0.4,     # mass distribution: realistic variation  
                0.3, 0.3, 0.8,     # stroke direction: realistic variation
                0.02, 0.1, 1.0,    # complexity: small realistic ranges
                0.8, 0.8, 0.5,     # spatial frequency: moderate variation
                0.5, 0.5, 0.5,     # moments: moderate shape variation
                0.03, 0.05         # density: very small realistic ranges
            ])
            expected_ranges = expected_ranges[:len(v1)]
            
            # Normalize differences by expected ranges and apply weights
            normalized_diffs = abs_diffs / expected_ranges
            weighted_normalized_diffs = weights * normalized_diffs
            
            # Calculate final similarity score
            total_weighted_diff = np.sum(weighted_normalized_diffs)
            max_possible_diff = np.sum(weights)  # Maximum possible difference with new ranges
            
            raw_similarity = 1 - (total_weighted_diff / max_possible_diff)
            
            # Apply VERY aggressive power scaling for maximum discrimination
            # Use higher power to push different signatures much lower
            final_similarity = max(0.0, raw_similarity) ** 3.0  # Increased from 2.0 to 3.0
            
            return max(0.0, min(1.0, final_similarity))
            
        elif feature_type == 'structural':
            # For structural features: Use weighted absolute difference
            # First 5 features are stroke widths (more important)
            # Next 6 features are density distribution
            # Last 4 features are endpoint/junction info
            
            if len(v1) >= 15:
                # Different weights for different types of structural features
                stroke_weights = np.array([3.0, 2.5, 2.0, 1.5, 1.5])      # Stroke width features
                density_weights = np.array([2.0, 2.0, 2.0, 2.0, 2.0, 2.0]) # Density features
                topo_weights = np.array([2.5, 2.5, 1.0, 1.0])              # Topology features
                
                weights = np.concatenate([stroke_weights, density_weights, topo_weights])
                weights = weights[:len(v1)]
                
                # Calculate weighted absolute differences
                abs_diffs = np.abs(v1 - v2)
                
                # Normalize by the maximum possible difference for each feature
                max_diffs = np.maximum(np.abs(v1), np.abs(v2)) + 1e-8
                normalized_diffs = abs_diffs / max_diffs
                
                # Apply weights
                weighted_diff = np.sum(weights * normalized_diffs) / np.sum(weights)
                similarity = max(0.0, 1 - weighted_diff)
                
                return similarity
            else:
                # Fallback for shorter vectors
                manhattan_dist = np.sum(np.abs(v1 - v2))
                max_manhattan = np.sum(np.abs(v1) + np.abs(v2)) + 1e-8
                return max(0.0, 1 - (manhattan_dist / max_manhattan))
            
        elif feature_type == 'texture':
            # For texture features: VERY aggressive discrimination for different signatures
            # Ensure non-negative values
            v1 = np.maximum(v1, 0) + 1e-10
            v2 = np.maximum(v2, 0) + 1e-10
            
            # Normalize to probability distributions
            v1_prob = v1 / np.sum(v1)
            v2_prob = v2 / np.sum(v2)
            
            # Calculate multiple highly discriminative measures
            
            # 1. Jensen-Shannon divergence (very sensitive)
            m = 0.5 * (v1_prob + v2_prob)
            kl1 = np.sum(v1_prob * np.log((v1_prob + 1e-10) / (m + 1e-10)))
            kl2 = np.sum(v2_prob * np.log((v2_prob + 1e-10) / (m + 1e-10)))
            js_div = 0.5 * (kl1 + kl2)
            js_similarity = 1 / (1 + js_div * 5)  # More aggressive scaling
            
            # 2. Very aggressive Chi-square distance
            chi_square = np.sum((v1_prob - v2_prob)**2 / (v1_prob + v2_prob + 1e-10))
            chi_similarity = 1 / (1 + chi_square * 10)  # Much more aggressive
            
            # 3. Bhattacharyya distance (very discriminative)
            bhattacharyya = -np.log(np.sum(np.sqrt(v1_prob * v2_prob)) + 1e-10)
            bhatta_similarity = np.exp(-bhattacharyya * 2)
            
            # 4. Total variation distance
            tv_distance = 0.5 * np.sum(np.abs(v1_prob - v2_prob))
            tv_similarity = 1 - tv_distance
            
            # Combine with heavy emphasis on most discriminative measures + power scaling
            combined = (0.3 * js_similarity + 0.3 * chi_similarity + 0.2 * bhatta_similarity + 0.2 * tv_similarity)
            
            # Apply very aggressive power scaling to push different signatures much lower
            final_similarity = combined ** 2.5
            
            return max(0.0, min(1.0, final_similarity))
            
        else:
            # Default: Use original cosine similarity logic (for CNN and backward compatibility)
            if isinstance(vector1, list) or isinstance(vector2, list):
                vector1 = np.array(vector1)
                vector2 = np.array(vector2)
            else:
                vector1 = v1.reshape(-1) if len(v1.shape) > 1 else v1
                vector2 = v2.reshape(-1) if len(v2.shape) > 1 else v2
                
            if len(vector1.shape) == 2:
                vector1 = vector1.flatten()
            if len(vector2.shape) == 2:
                vector2 = vector2.flatten()
                
            min_length = min(len(vector1), len(vector2))
            vector1 = vector1[:min_length]
            vector2 = vector2[:min_length]
                
            if np.all(vector1 == 0) or np.all(vector2 == 0):
                return 0.0
                
            try:
                return 1 - cosine(vector1, vector2)
            except Exception as e:
                print(f"Error calculating similarity: {e}")
                return 0.0

    # Added function to match the preprocessing from the original system
    def preprocess_signature(self, image_path):
        """
        Preprocess signature image in the same way as signature_verification_system.py
        """
        try:
            # Read image
            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
            if img is None:
                raise ValueError(f"Failed to load image: {image_path}")
                
            # Apply preprocessing steps to match the original implementation
            # 1. Normalize
            img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)
            
            # 2. Binarize using Otsu's method
            _, binary = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
            
            # 3. Resize to 128x128 (matching the original system)
            processed = cv2.resize(binary, (128, 128))
            
            # 4. Add channel dimension and normalize to [0,1]
            processed = processed.astype('float32') / 255.0
            processed = np.expand_dims(processed, axis=-1)
            
            return processed
        except Exception as e:
            print(f"Error in preprocessing signature: {e}")
            # Return empty image of correct dimensions
            return np.zeros((128, 128, 1), dtype='float32')

    # Added function to match the Siamese comparison from the original system
    def compare_siamese_images(self, reference_path, test_path):
        """
        Compare two images using the Siamese network in the same way
        as signature_verification_system.py
        """
        if self.siamese_network is None:
            print("Siamese model not loaded. Returning 0.")
            return 0.0
        
        try:
            # Preprocess images using the same preprocessing as the original
            reference_img = self.preprocess_signature(reference_path)
            test_img = self.preprocess_signature(test_path)
            
            # Reshape for batch dimension if needed
            reference_img = np.expand_dims(reference_img, axis=0)
            test_img = np.expand_dims(test_img, axis=0)
            
            # Predict similarity with the same format as original system
            similarity = self.siamese_network.predict([reference_img, test_img], verbose=0)[0][0]
            
            return float(similarity)
        except Exception as e:
            print(f"Error in Siamese comparison: {e}")
            return 0.0

    def preprocessing_lstm(self, img_path):
        """
        LSTM-specific preprocessing - exactly matching lstm.py implementation
        """
        return self.improved_preprocessor.extract_enhanced_features(img_path, feature_dim=16)

    def calculate_improved_lstm_similarity(self, genuine_path, test_path):
        """
        Calculate LSTM similarity using the EXACT same method as lstm.py
        This matches the comprehensive similarity calculation from the standalone lstm.py
        """
        if self.improved_lstm_model is None:
            print("Improved LSTM model not loaded. Returning 0.")
            return 0.0
        
        try:
            print(f"\nüîç IMPROVED LSTM SIMILARITY CALCULATION")
            print(f"   Genuine: {os.path.basename(genuine_path)}")
            print(f"   Test: {os.path.basename(test_path)}")
            
            # Extract enhanced features using the EXACT same preprocessing as lstm.py
            print("üìä Extracting enhanced features...")
            signature1_features = self.preprocessing_lstm(genuine_path)
            signature2_features = self.preprocessing_lstm(test_path)
            print(f"   Feature shape: {signature1_features.shape}")
            
            # Apply scaling if scaler is available - EXACT same way as lstm.py
            if self.lstm_scaler:
                print("üìè Applying feature standardization...")
                # Reshape for scaling
                features1_flat = signature1_features.reshape(-1, 16)
                features2_flat = signature2_features.reshape(-1, 16)
                
                # Apply scaling
                features1_scaled = self.lstm_scaler.transform(features1_flat)
                features2_scaled = self.lstm_scaler.transform(features2_flat)
                
                # Reshape back
                signature1_features = features1_scaled.reshape(128, 16)
                signature2_features = features2_scaled.reshape(128, 16)
                print("‚úì Features standardized using training scaler")
            else:
                print("‚ö† No scaler available - using raw features")
            
            # Prepare data for model prediction - EXACT same way as lstm.py
            X1 = np.expand_dims(signature1_features, axis=0)
            X2 = np.expand_dims(signature2_features, axis=0)
            
            # Get model predictions - EXACT same way as lstm.py
            print("ü§ñ Running improved LSTM model predictions...")
            pred1 = self.improved_lstm_model.predict(X1, verbose=0)[0][0]
            pred2 = self.improved_lstm_model.predict(X2, verbose=0)[0][0]
            print(f"   Signature 1 Authenticity: {pred1:.4f}")
            print(f"   Signature 2 Authenticity: {pred2:.4f}")
            
            # Calculate enhanced similarities - EXACT same way as lstm.py
            print("üìà Calculating enhanced similarity metrics...")
            
            # Cosine similarity
            cosine_sim = cosine_similarity([signature1_features.flatten()], 
                                         [signature2_features.flatten()])[0][0]
            
            # Euclidean distance (normalized)
            euclidean_dist = euclidean(signature1_features.flatten(), 
                                     signature2_features.flatten())
            max_possible_dist = np.sqrt(2 * len(signature1_features.flatten()))
            euclidean_similarity = 1 - (euclidean_dist / max_possible_dist)
            
            # Feature correlation
            correlation = np.corrcoef(signature1_features.flatten(), 
                                    signature2_features.flatten())[0, 1]
            if np.isnan(correlation):
                correlation = 0.0
            
            # Feature-wise similarities (for each of the 16 features)
            feature_similarities = []
            for i in range(16):
                feat_sim = cosine_similarity([signature1_features[:, i]], 
                                           [signature2_features[:, i]])[0][0]
                feature_similarities.append(feat_sim)
            
            avg_feature_similarity = np.mean(feature_similarities)
            
            # Overall similarity score (weighted for improved model) - EXACT same as lstm.py
            prediction_similarity = 1 - abs(pred1 - pred2)  # How similar the predictions are
            overall_similarity = (cosine_sim * 0.3 + euclidean_similarity * 0.2 + 
                                abs(correlation) * 0.2 + avg_feature_similarity * 0.2 + 
                                prediction_similarity * 0.1)
            
            # Debug output to match lstm.py
            print(f"üìä Enhanced Similarity Metrics:")
            print(f"   Global Cosine Similarity: {cosine_sim:.4f}")
            print(f"   Euclidean Similarity: {euclidean_similarity:.4f}")
            print(f"   Feature Correlation: {correlation:.4f}")
            print(f"   Average Feature Similarity: {avg_feature_similarity:.4f}")
            print(f"   Prediction Similarity: {prediction_similarity:.4f}")
            print(f"üéñÔ∏è  Overall Similarity Score: {overall_similarity:.4f}")
            
            return float(overall_similarity)
            
        except Exception as e:
            print(f"Error in improved LSTM calculation: {e}")
            import traceback
            traceback.print_exc()
            return 0.0

    def verify_signature(self, genuine_path, test_path, debug=True):
        """Verify signature using multi-model approach with FIXED similarity calculations"""
        try:
            # Start timing
            start_time = time.time()
            timings = {}
            
            # Extract and compare traditional features with FIXED similarity calculation
            similarities = {}
            
            # CNN uses default (original) similarity calculation
            feature_start = time.time()
            genuine_features = self.feature_extractor.extract_cnn_features(genuine_path)
            test_features = self.feature_extractor.extract_cnn_features(test_path)
            similarities['cnn'] = self.calculate_similarity(genuine_features, test_features)  # No feature type = original behavior
            timings['cnn'] = time.time() - feature_start
            if debug:
                print(f"cnn similarity: {similarities['cnn']:.3f}")
            
            # Other features use new fixed similarity calculations
            for feature_type in ['texture', 'geometric', 'structural']:
                feature_start = time.time()
                extract_method = getattr(self.feature_extractor, f'extract_{feature_type}_features')
                genuine_features = extract_method(genuine_path)
                test_features = extract_method(test_path)
                
                # Use the FIXED similarity calculation with feature type
                similarities[feature_type] = self.calculate_similarity(
                    genuine_features, test_features, feature_type
                )
                
                timings[feature_type] = time.time() - feature_start
                
                if debug:
                    print(f"{feature_type} similarity: {similarities[feature_type]:.3f}")

            # Process LSTM features using the improved model
            lstm_start = time.time()
            lstm_similarity = self.calculate_improved_lstm_similarity(genuine_path, test_path)
            timings['lstm'] = time.time() - lstm_start
            
            if debug:
                print(f"Improved LSTM similarity: {lstm_similarity:.3f}")

            # Process Siamese similarity - using the new method that matches original system
            siamese_start = time.time()
            siamese_similarity = self.compare_siamese_images(genuine_path, test_path)
            
            if debug:
                print(f"Siamese similarity: {siamese_similarity:.3f}")
                
            timings['siamese'] = time.time() - siamese_start

            # Calculate weighted similarity
            weights_sum = sum(self.weights.values())
            weighted_similarity = (
                similarities['cnn'] * self.weights['cnn'] +
                similarities['texture'] * self.weights['texture'] +
                similarities['geometric'] * self.weights['geometric'] +
                similarities['structural'] * self.weights['structural'] +
                lstm_similarity * self.weights['lstm'] +
                siamese_similarity * self.weights['siamese']
            ) / weights_sum
            
            # Make verification decision using the same threshold as original
            is_genuine = weighted_similarity > self.threshold

            # Total processing time
            total_time = time.time() - start_time

            if debug:
                print(f"\nFinal Results:")
                print(f"Weighted similarity: {weighted_similarity:.3f}")
                print(f"Verdict: {'Genuine' if is_genuine else 'Forged'}")
                print(f"Total processing time: {total_time:.3f} seconds")

            return {
                'cnn_similarity': float(similarities['cnn']),
                'texture_similarity': float(similarities['texture']),
                'geometric_similarity': float(similarities['geometric']),
                'structural_similarity': float(similarities['structural']),
                'lstm_similarity': float(lstm_similarity),
                'siamese_similarity': float(siamese_similarity),
                'weighted_similarity': float(weighted_similarity),
                'is_genuine': is_genuine,
                'confidence': float(weighted_similarity),
                'timing': timings,
                'total_time': total_time
            }

        except Exception as e:
            print(f"Error in verification process: {e}")
            import traceback
            traceback.print_exc()
            return None

    def load_models(self, lstm_path, siamese_path):
        """
        Load both improved LSTM and Siamese models
        Automatically looks for improved_lstm_model.keras in same directory as siamese model
        """
        try:
            # Handle automatic LSTM path detection
            if siamese_path and os.path.exists(siamese_path):
                # Get directory of siamese model
                siamese_dir = os.path.dirname(siamese_path)
                auto_lstm_path = os.path.join(siamese_dir, "improved_lstm_model.keras")
                auto_scaler_path = os.path.join(siamese_dir, "improved_lstm_model_scaler.pkl")
                
                # ALWAYS use auto-detected path for improved LSTM (ignore lstm_path parameter)
                if os.path.exists(auto_lstm_path):
                    lstm_path = auto_lstm_path
                    print(f"Auto-detected improved LSTM model: {auto_lstm_path}")
                    
                    # Also try to load the scaler
                    if os.path.exists(auto_scaler_path):
                        try:
                            self.lstm_scaler = joblib.load(auto_scaler_path)
                            print(f"Auto-detected and loaded scaler: {auto_scaler_path}")
                        except Exception as e:
                            print(f"Could not load scaler: {e}")
                    else:
                        print(f"Scaler not found at: {auto_scaler_path}")
                else:
                    print(f"Improved LSTM model not found at: {auto_lstm_path}")
            
            # Load improved LSTM model if path provided or auto-detected
            if lstm_path and os.path.exists(lstm_path):
                try:
                    print(f"Loading improved LSTM model from {lstm_path}")
                    self.improved_lstm_model = load_model(lstm_path)
                    print(f"Improved LSTM model loaded successfully")
                    print(f"Model input shape: {self.improved_lstm_model.input_shape}")
                    print(f"Model output shape: {self.improved_lstm_model.output_shape}")
                    
                    # Verify this is the correct improved model (should be 128, 16)
                    expected_shape = (None, 128, 16)
                    if self.improved_lstm_model.input_shape == expected_shape:
                        print("‚úì Confirmed: This is the improved LSTM model with correct input shape")
                    else:
                        print(f"‚ö† WARNING: Expected input shape {expected_shape}, got {self.improved_lstm_model.input_shape}")
                        print("This might be the old LSTM model, not the improved one!")
                        
                except Exception as e:
                    print(f"Error loading improved LSTM model: {e}")
                    import traceback
                    traceback.print_exc()
            else:
                print("Improved LSTM model path not provided or file does not exist")
                print(f"Checked path: {lstm_path}")
            
            # Load Siamese model if path provided
            if siamese_path and os.path.exists(siamese_path):
                try:
                    print(f"Loading Siamese model from {siamese_path}")
                    custom_objects = {
                        'euclidean_distance': euclidean_distance
                    }
                    self.siamese_network = load_model(siamese_path, custom_objects=custom_objects)
                    print("Siamese model loaded successfully")
                except Exception as e:
                    print(f"Error loading Siamese model: {e}")
            else:
                print("Siamese model path not provided or file does not exist")
                
        except Exception as e:
            print(f"Error in load_models: {e}")
            import traceback
            traceback.print_exc()